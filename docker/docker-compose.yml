name: stt-faster
services:

  # ---------- Local LLM server ---------------------------------------------------
  ollama:
    image: ${OLLAMA_IMAGE:-ollama/ollama:latest}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ../model_cache:/root/.ollama
    ports:
      - "127.0.0.1:11434:11434"
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  # ---------- App Container -----------------------------------------
  app:
    image: app
    build:
      context: ..
      dockerfile: docker/app.Dockerfile
      target: runtime
      # Allow reproducible base images while keeping overrides simple.
      # Defaults mirror the digest-pinned ARGs in the Dockerfile.
      args:
        UV_IMAGE_REF: ${UV_IMAGE_REF:-ghcr.io/astral-sh/uv:python3.12-bookworm-slim@sha256:c6fad5e08092142ea53fc03cbebb6304c5d06a25dd53ab450114f6c5f03376a7}
        PYTHON_RUNTIME_IMAGE: ${PYTHON_RUNTIME_IMAGE:-python:3.12-slim-bookworm@sha256:3ad2a947749a3eb74acd9e00636ffa0def5aae0bbbd9fa4fff6253e404e2fe15}
    volumes:
      - ../backend:/app/backend # Live backend code
      - ../frontend:/app/frontend # Live frontend code
      - ../logs:/app/logs # Live log access
      - ../tests:/app/tests # Mount tests for in-container testing
      - ../scripts:/app/scripts # Shell helpers used by tests
      - ../docker:/app/docker # Compose file presence checked by scripts/common.sh
      - ../reports:/app/reports # Persist test reports to host
    working_dir: /app
    # Start minimal backend keepalive entrypoint
    command: ["python", "-m", "backend.main"]
    healthcheck:
      test: ["CMD", "python", "-m", "backend.main", "--healthcheck"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 30s
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      # Point the Python code to the other services inside the compose network
      - OLLAMA_URL=http://ollama:11434
      - APP_LOG_DIR=/app/logs
